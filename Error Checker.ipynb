{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set db_folder and laws_and_regs constants\n",
    "db_folder = r'db'\n",
    "laws_and_regs_f = 'laws_and_regs.csv'\n",
    "laws_and_regs =  pd.read_csv(laws_and_regs_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check for failed acts (empty content; empty revoked regs)\n",
    "def check_jsons_act_content(json_folder):\n",
    "    # Define the column names\n",
    "    columns = ['citation', 'url']\n",
    "    data = []\n",
    "\n",
    "    for file in os.listdir(json_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            try:\n",
    "                with open(os.path.join(json_folder, file), 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "\n",
    "                    # Using get method with default values\n",
    "                    revoked_regs = json_data.get('revoked_regs', None)\n",
    "                    versions = json_data.get('versions', [])\n",
    "                    content = json_data.get('content', None)\n",
    "\n",
    "                    # Check if 'act_info' is present\n",
    "                    if 'act_info' in json_data:\n",
    "                        # Conditions for including the data\n",
    "                        include_data = (content is None) or (revoked_regs is None and len(versions) < 3)\n",
    "\n",
    "                        if include_data:\n",
    "                            data.append({\n",
    "                                'citation': json_data['act_info']['full_title'],\n",
    "                                'url': json_data['act_info']['url']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "        #print(f\"Processing {file}\")\n",
    "\n",
    "    return pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check for failed regs, empty content\n",
    "def check_jsons_reg_content(json_folder):\n",
    "    data = []\n",
    "\n",
    "    for file in os.listdir(json_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(json_folder, file), 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "                # Check if 'reg_info' is present and 'content' is empty\n",
    "                if 'reg_info' in json_data and not json_data.get('content'):\n",
    "                    data.append({\n",
    "                        'citation': json_data['reg_info']['full_title'],\n",
    "                        'url': json_data['reg_info']['url']\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make df of all JSON files to check for missed items\n",
    "def get_missed_items(json_folder, laws_and_regs):\n",
    "    \"\"\"\n",
    "    Reads JSON files from a specified folder, extracts specific information from each file, \n",
    "    and then merges this data with a predefined DataFrame 'laws_and_regs' to identify \n",
    "    items present in 'laws_and_regs' but not in the JSON files.\n",
    "\n",
    "    Each JSON file is expected to contain either 'act_info' or 'reg_info' from which \n",
    "    a citation and a URL are extracted. The function normalizes URLs in 'laws_and_regs' \n",
    "    by appending a base URL to the 'ahref' field and then performs an outer merge based \n",
    "    on these URLs to find any missed items.\n",
    "\n",
    "    Parameters:\n",
    "    json_folder (str): The file path to the folder containing the JSON files.\n",
    "    laws_and_regs: The laws_and_regs file containing list to be scraped\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame of missed items, which are present in 'laws_and_regs' \n",
    "    but not in the JSON files. The returned DataFrame excludes the columns 'type', \n",
    "    'citation_x', 'url', and 'full_url'.\n",
    "\n",
    "    Example Usage:\n",
    "    missed_items = get_missed_items(r'C:\\\\Path\\\\To\\\\Json\\\\Folder', laws_and_regs_df)\n",
    "    print(all_jsons)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for file in os.listdir(json_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(json_folder, file), 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "                # Check if 'act_info' or 'reg_info' is present and extract relevant data\n",
    "                if 'act_info' in json_data:\n",
    "                    citation = json_data['act_info'].get('full_title', 'No Title')\n",
    "                    url = json_data['act_info'].get('url', 'No URL')\n",
    "                    data.append({'type': 'Act', 'citation': citation, 'url': url})\n",
    "                elif 'reg_info' in json_data:\n",
    "                    citation = json_data['reg_info'].get('full_title', 'No Title')\n",
    "                    url = json_data['reg_info'].get('url', 'No URL')\n",
    "                    data.append({'type': 'Regulation', 'citation': citation, 'url': url})\n",
    "\n",
    "    #DF Data\n",
    "    all_jsons = pd.DataFrame(data)\n",
    "\n",
    "    # Normalize the 'ahref' in laws_and_regs to full URLs by prepending the base URL\n",
    "    base_url = 'https://www.ontario.ca'\n",
    "    laws_and_regs['url'] = base_url + laws_and_regs['ahref'].astype(str)\n",
    "\n",
    "    # Merge on 'ahref' from df_regs and 'url' from laws_and_regs\n",
    "    all_df = all_jsons.merge(laws_and_regs, how='outer', on=\"url\", indicator=True)\n",
    "    missed_items = all_df[all_df['_merge'] == 'right_only'].copy()\n",
    "\n",
    "    #Rename Citation column\n",
    "    missed_items.loc[:, 'citation'] = missed_items['citation_y']\n",
    "\n",
    "    #Drop Excess columns\n",
    "    missed_items = missed_items.drop(['type', 'citation_x', 'url', '_merge', 'citation_y'], axis=1)\n",
    "\n",
    "    #Reorder columns\n",
    "    missed_items = missed_items[['ahref', \n",
    "                                'citation', \n",
    "                                'class', \n",
    "                                'parent_legislation', \n",
    "                                'currency', \n",
    "                                'currency_date',\n",
    "                                'date_scraped']]\n",
    "\n",
    "    return missed_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Check for Errors and Save rescrape_list.csv\n",
    "def check_for_fails(db_folder, laws_and_regs):\n",
    "    \"\"\"\n",
    "    Scans the database folder and compares it with the laws and regulations file to identify any missing or failed downloads.\n",
    "    This function is designed to create a list of items that need to be re-scraped due to errors or omissions.\n",
    "\n",
    "    Args:\n",
    "    db_folder (str): The path to the database folder where acts and regulations are stored.\n",
    "    laws_and_regs (str): The filename of the laws and regulations CSV file. Default is 'laws_and_regs.csv'.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    - Reads the laws and regulations CSV file into a DataFrame.\n",
    "    - Checks for errors in acts and regulations content using helper functions.\n",
    "    - Identifies any items that were missed during the initial scraping.\n",
    "    - Normalizes URLs in the laws_and_regs DataFrame for comparison.\n",
    "    - Merges data to create a final list of items to re-scrape.\n",
    "    - Saves this list as 'rescrape_list.csv' in a 'failed' subdirectory within the specified db_folder.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    db_folder = 'path/to/db_folder'\n",
    "    laws_and_regs = 'laws_and_regs.csv'\n",
    "    check_for_fails(db_folder, laws_and_regs)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    #Make laws_and_regs DF\n",
    "    laws_and_regs = pd.read_csv(laws_and_regs)\n",
    "\n",
    "    #Compile 3 basic lists, error acts, error regs, and missed items\n",
    "    df_acts = check_jsons_act_content(db_folder)\n",
    "    df_regs = check_jsons_reg_content(db_folder)\n",
    "    missed_items = get_missed_items(db_folder, laws_and_regs)\n",
    "\n",
    "    # Normalize the 'ahref' in laws_and_regs to full URLs by prepending the base URL\n",
    "    base_url = 'https://www.ontario.ca'\n",
    "    laws_and_regs['url'] = base_url + laws_and_regs['ahref']\n",
    "\n",
    "    # Perfom merges to get final errored rescrape_list\n",
    "    acts_rescrape_list = df_acts.merge(laws_and_regs, left_on='url', right_on='url', how='left')\n",
    "    regs_rescrape_list = df_regs.merge(laws_and_regs, left_on='url', right_on='url', how='left')\n",
    "    raw_rescrape_list = acts_rescrape_list.merge(regs_rescrape_list, how=\"outer\")\n",
    "        \n",
    "    #Rename Citation column\n",
    "    raw_rescrape_list.loc[:, 'citation'] = raw_rescrape_list['citation_y']\n",
    "\n",
    "    #Drop & Merge with missed items for final rescraping list\n",
    "    raw_rescrape_list = raw_rescrape_list.drop(['citation_x', 'url', 'citation_y'], axis=1)\n",
    "    full_rescrape_list = raw_rescrape_list.merge(missed_items, how=\"outer\")\n",
    "\n",
    "    #Reorder columns\n",
    "    full_rescrape_list = full_rescrape_list[['ahref', \n",
    "                            'citation', \n",
    "                            'class', \n",
    "                            'parent_legislation', \n",
    "                            'currency', \n",
    "                            'currency_date',\n",
    "                            'date_scraped']]\n",
    "\n",
    "    # Define save path\n",
    "    save_path = os.path.join('failed', 'rescrape_list.csv')\n",
    "\n",
    "    # Check if the directory exists, and create it if it does not\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Save CSV\n",
    "    full_rescrape_list.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f'File saved to {save_path}')\n",
    "\n",
    "check_for_fails(db_folder, laws_and_regs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
