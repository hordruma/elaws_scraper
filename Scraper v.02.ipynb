{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constant for requests headers\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Classes\n",
    "SECTION_CLASSES = ['section', 'section-e']\n",
    "HEADNOTE_CLASSES = ['headnote', 'headnote-e']\n",
    "AMENDMENTS_HEADING_CLASS = 'amendments-heading'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to contain base URLs\n",
    "def get_url_dict():\n",
    "    '''\n",
    "    ##List of main URLs for different datasets, returns a dictionary of urls for different configurations:\n",
    "    all_source_law - all \"Source Law\" including priv. statutes, total 13.75K (as of Dec 7, 2023)\n",
    "    all_law - Current and Repealed Laws & regs, total 5088 (as of Dec 7 2023)\n",
    "    current_all_law - current laws & regs, total 3023 (as of Dec 7 2023)\n",
    "    repealed_all_law - repealed laws & regs, total 2065 (as of Dec 7 2023)\n",
    "    current_statutes - current laws, total 816 (as of Dec 7 2023)\n",
    "    current_regs - current regs, total 2207 (as of Dec 7 2023)\n",
    "    repealed_statutes - repealed laws, total 408 (as of Dec 7 2023)\n",
    "    repealed_regs - repealed regs, total 1657 (as of Dec 7 2023)\n",
    "\n",
    "    '''\n",
    "    # ELaws URL Constants\n",
    "    url_dict = {\n",
    "        \"all_source_law\": \"https://www.ontario.ca/laws?search=&filterstate%5B%5D=current&filteroption=source&filteryear=&source_type%5B%5D=public&source_type%5B%5D=private&source_type%5B%5D=regulation&pit_date=&filtertype=Regulation&sort=&sort_letter=&browse=on\",\n",
    "        \"all_law\": \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B0%5D=current&filterstate%5B1%5D=rrs&filteryear=&source_type%5B0%5D=public&source_type%5B1%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on&from=\",\n",
    "        \"current_all_law\": \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B%5D=current&filteryear=&source_type%5B%5D=public&source_type%5B%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on\",\n",
    "        \"repealed_all_law\": \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B%5D=rrs&filteryear=&source_type%5B%5D=public&source_type%5B%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on\",\n",
    "        \"current_statutes\": \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B%5D=current&filteryear=&source_type%5B%5D=public&source_type%5B%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on\",\n",
    "        \"current_regs\": \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B%5D=rrs&filteryear=&source_type%5B%5D=public&source_type%5B%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on\",\n",
    "        \"repealed_statutes\": \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B%5D=current&filteryear=&source_type%5B%5D=public&source_type%5B%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on\",\n",
    "        \"repealed_regs\": \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B%5D=rrs&filteryear=&source_type%5B%5D=public&source_type%5B%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on\",\n",
    "        # ... other URLs\n",
    "    }\n",
    "    return url_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select URL for relevant dataset\n",
    "def get_main_base_url(key=\"all_law\"):\n",
    "    '''\n",
    "    #Gets the url for a particular scraper batch, options include:\n",
    "    all_source_law - all \"Source Law\" including priv. statutes, total 13.75K (as of Dec 7, 2023)\n",
    "    all_law - Current and Repealed Laws & regs, total 5088 (as of Dec 7 2023)\n",
    "    current_all_law - current laws & regs, total 3023 (as of Dec 7 2023)\n",
    "    repealed_all_law - repealed laws & regs, total 2065 (as of Dec 7 2023)\n",
    "    current_statutes - current laws, total 816 (as of Dec 7 2023)\n",
    "    current_regs - current regs, total 2207 (as of Dec 7 2023)\n",
    "    repealed_statutes - repealed laws, total 408 (as of Dec 7 2023)\n",
    "    repealed_regs - repealed regs, total 1657 (as of Dec 7 2023)\n",
    "\n",
    "    ## Example usage\n",
    "    #print(main_base_url(\"current_all_law\"))  # Should return the URL for 'current_all_law'\n",
    "    '''\n",
    "    url_dict = get_url_dict()\n",
    "    main_base_url = url_dict.get(key, \"Key not found\")\n",
    "    return main_base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to start Selenium driver, get URL, and wait\n",
    "'''\n",
    "#Initializes FireFox webdriver and gets the current URL, waits 10s so all elements are loaded.\n",
    "args = url, wait_time (default 10s)\n",
    "'''\n",
    "def start_driver_and_wait(url, wait_time=8):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for a fixed period\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fetch and parse page html using BS\n",
    "def fetch_and_parse(driver):\n",
    "    \"\"\"\n",
    "    Parses the current page content of a WebDriver into a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver): Selenium WebDriver with the loaded page.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML content of the page.\n",
    "\n",
    "    ###Example USG:\n",
    "    #driver = start_driver_and_wait(url)\n",
    "    #soup = fetch_and_parse(driver)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to get list of laws and regs, 5086 Available as of 2023-11-20; default: (0, 6000, 50)\n",
    "def scrape_ontario_laws(main_base_url, start_page=0, end_page=6000, step=50):\n",
    "    \"\"\"\n",
    "    Scrapes a specified range of pages from the Ontario laws website for legislation and regulation information.\n",
    "\n",
    "    The function iterates over pages in the specified range, extracting data such as hyperlinks, citations,\n",
    "    classes (act or regulation), parent legislation names, currency status, and currency dates. The extracted \n",
    "    data is then compiled into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    main_base_url (str): The base URL of the Ontario laws website to scrape.\n",
    "    start_page (int, optional): The starting page number for scraping. Default is 0.\n",
    "    end_page (int, optional): The ending page number for scraping. Default is 6000.\n",
    "    step (int, optional): The step size for iterating through the pages. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the scraped data, with columns for hyperlinks ('ahref'), citations,\n",
    "    class ('regulation' or 'act'), parent legislation, currency status, currency date, and the date scraped.\n",
    "\n",
    "    Example:\n",
    "    # Example usage\n",
    "    main_base_url = \"https://www.ontario.ca/laws?...\"\n",
    "    laws_and_regs = scrape_ontario_laws(main_base_url, start_page=0, end_page=6000, step=50)\n",
    "    print(laws_and_regs.shape)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'laws_and_regs.csv'\n",
    "    laws_and_regs.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "    \"\"\"\n",
    "    # Create lists to store the extracted data\n",
    "    ahref_list = []\n",
    "    citation_list = []\n",
    "    class_list = []\n",
    "    parent_legislation_list = []\n",
    "    currency_list = []\n",
    "    currency_date_list = []\n",
    "\n",
    "    #Scrape legislation and regulations - \n",
    "        # Scrape legislation and regulations\n",
    "    for page_number in range(start_page, end_page, step):\n",
    "        URL = main_base_url + str(page_number)\n",
    "        response = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        #Print the page URL for verification:\n",
    "            print(\"Scraping page:\", URL)\n",
    "\n",
    "            # Find all the table rows (tr)\n",
    "        rows = soup.find_all('tr')\n",
    "        print(\"Number of rows in page:\", len(rows))\n",
    "\n",
    "        # Iterate through each row\n",
    "        for row in rows:\n",
    "            # Find the td within the row\n",
    "            td = row.find('td')\n",
    "\n",
    "            if td:\n",
    "                # Extract the information from the td\n",
    "                a_tag = td.find('a')\n",
    "                ahref = a_tag['href'] if a_tag else None\n",
    "                citation = a_tag.get_text(strip=True) if a_tag else None\n",
    "\n",
    "                div_reg_act = td.find('div', class_='reg-act')\n",
    "                parent_legislation = div_reg_act.find('b').get_text() if div_reg_act else \"None\"\n",
    "\n",
    "                span_label = td.find('span', class_='label')\n",
    "                currency = span_label.get_text(strip=True) if span_label else None\n",
    "\n",
    "                # Find the span with class 'time' or 'no-date'\n",
    "                span_time = td.find('span', class_='time')\n",
    "                if span_time:\n",
    "                    currency_date = span_time.get_text(strip=True)\n",
    "                else:\n",
    "                    span_no_date = td.find('span', class_='no-date')\n",
    "                    if span_no_date:\n",
    "                        currency_date = span_no_date.get_text(strip=True)\n",
    "                    else:\n",
    "                        currency_date = \"None\"\n",
    "\n",
    "                # Append the extracted data to the respective lists\n",
    "                ahref_list.append(ahref)\n",
    "                citation_list.append(citation)\n",
    "                class_list.append(\"regulation\" if div_reg_act else \"act\")\n",
    "                parent_legislation_list.append(parent_legislation)\n",
    "                currency_list.append(currency)\n",
    "                currency_date_list.append(currency_date)\n",
    "\n",
    "        # else:\n",
    "            #  print(f\"Failed to retrieve page {URL}\")\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    data = {\n",
    "        'ahref': ahref_list,\n",
    "        'citation': citation_list,\n",
    "        'class': class_list,\n",
    "        'parent_legislation': parent_legislation_list,\n",
    "        'currency': currency_list,\n",
    "        'currency_date': currency_date_list\n",
    "    }\n",
    "\n",
    "    laws_and_regs = pd.DataFrame(data)\n",
    "\n",
    "    #Add Date Scraped Column\n",
    "    today = datetime.today()\n",
    "    laws_and_regs['date_scraped'] = today\n",
    "\n",
    "    # Drop duplicate rows if any\n",
    "    laws_and_regs = laws_and_regs.drop_duplicates(keep='first')\n",
    "\n",
    "    return laws_and_regs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to sanitize filenames\n",
    "def sanitize_filename(name, max_length=168):\n",
    "    if isinstance(name, pd.Series):\n",
    "        name = name.iloc[0] if not name.empty else \"default_name\"\n",
    "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
    "    sanitized_name = ''.join(c for c in name if c in valid_chars)\n",
    "    return sanitized_name[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Scrape Versions to csv\n",
    "def scrape_versions_to_csv(url, driver, identifier, output_directory):\n",
    "    \"\"\"\n",
    "    Scrapes version information from a url/law and saves it as a CSV file.\n",
    "\n",
    "    This function navigates to a given URL using a Selenium WebDriver, extracts version data from a specified HTML structure,\n",
    "    and saves the data to a CSV file in a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): Webpage URL to scrape.\n",
    "    driver (webdriver object): Selenium WebDriver for web navigation.\n",
    "    identifier (str): Identifier for naming the output file.\n",
    "    output_directory (str): Directory path for saving the CSV file.\n",
    "    \n",
    "    # Example usage\n",
    "    scrape_versions_to_csv(\"https://www.ontario.ca/laws/statute/90l11\", 'test', 'c:/XX')\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        try:\n",
    "            link = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tr.more-versions.hide a\")))\n",
    "            link.click()\n",
    "        except:\n",
    "            print(\"More Versions link not found. Proceeding with the current page source.\")\n",
    "\n",
    "        #Make soup\n",
    "        soup = fetch_and_parse(driver)\n",
    "\n",
    "        # Find the 'div' tag with the class \"versions\"\n",
    "        versions = soup.find('div', id=\"versions\")\n",
    "        if not versions:\n",
    "            print(\"Versions Tag not found.\")\n",
    "            return\n",
    "\n",
    "        # Find the table and get rows\n",
    "        versions_table = versions.find('table', class_='act-reg-list noStripes')\n",
    "        if not versions_table:\n",
    "            print(\"Table not found in the HTML.\")\n",
    "            return\n",
    "\n",
    "        version_rows = versions_table.findAll('tr')\n",
    "        if not version_rows:\n",
    "            print(\"No rows found in the table.\")\n",
    "            return\n",
    "\n",
    "        # Initialize lists to store the data\n",
    "        hrefs, valid_froms, valid_tos = [], [], []\n",
    "\n",
    "        # Process each row\n",
    "        for row in version_rows:\n",
    "                td_cells = row.find_all('td')\n",
    "                if len(td_cells) >= 2:\n",
    "                    a_tag = td_cells[1].find('a')\n",
    "                    if a_tag:\n",
    "                        span_tags = td_cells[1].find_all('span', class_='time')\n",
    "                        href = a_tag['href']\n",
    "                        valid_from = span_tags[0].get_text().strip() if span_tags else 'N/A'\n",
    "                        valid_to = span_tags[1].get_text().strip() if len(span_tags) > 1 else 'current'\n",
    "                        hrefs.append(href)\n",
    "                        valid_froms.append(valid_from)\n",
    "                        valid_tos.append(valid_to)\n",
    "\n",
    "        # Create DataFrame and save to CSV\n",
    "        versions_data = pd.DataFrame({\n",
    "            'a_href': hrefs,\n",
    "            'valid_from': valid_froms,\n",
    "            'valid_to': valid_tos\n",
    "        })\n",
    "\n",
    "        # Normalize Full-URL\n",
    "        current_url = url\n",
    "        relative_path = current_url.replace('https://www.ontario.ca', '')\n",
    "\n",
    "        # Replace the first value in 'a_href' column\n",
    "        versions_data.loc[0, 'a_href'] = relative_path\n",
    "\n",
    "        # Ensure output_directory exists\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "\n",
    "        # Process identifier to ensure it is a valid filename\n",
    "        valid_filename = sanitize_filename(identifier)\n",
    "        csv_filename = f\"{valid_filename}.csv\"\n",
    "        \n",
    "\n",
    "        # Saving the file\n",
    "        file_path = os.path.join(output_directory, csv_filename)\n",
    "        try:\n",
    "            versions_data.to_csv(file_path, index=False)\n",
    "            print(f\"Data saved to {csv_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save {csv_filename}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract revoked regs:\n",
    "def extract_revoked_regs_data(soup, url):\n",
    "    \"\"\"\n",
    "    Extracts data about revoked regulations from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): BeautifulSoup object of the page.\n",
    "        url (str): The URL of the web page being processed, used for logging and tracking purposes.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with data of revoked regulations.\n",
    "    # Example usage\n",
    "    #url = \"https://www.ontario.ca/laws/statute/90p10\"\n",
    "    #revoked_regs_data = extract_revoked_regs_data(url)\n",
    "\n",
    "    # Print first few JSON objects for demonstration\n",
    "    #for json_obj in revoked_regs_data[:5]:\n",
    "    #    print(json_obj)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize Revoked Regs DF\n",
    "    revoked_regs_data = pd.DataFrame()\n",
    "\n",
    "    # Find the 'div' tag with the class \"versions\"\n",
    "    revoked_regs = soup.find('div', id=\"revoked_regulations\")\n",
    "\n",
    "    if revoked_regs is not None:\n",
    "    #print(\"Revoked Regulations (RR) Tag found for \" + url + \"!\")\n",
    "\n",
    "        # Find the table\n",
    "        revoked_regs_table = revoked_regs.find('table', class_='act-reg-list noStripes')\n",
    "\n",
    "        if revoked_regs_table:\n",
    "            revoked_regs_rows = revoked_regs_table.findAll('tr')\n",
    "            if revoked_regs_rows:\n",
    "            # print(\"Rows found!\")\n",
    "                # Process the rows\n",
    "                citations, titles, hrefs = [], [], []\n",
    "                \n",
    "                for row in revoked_regs_rows:\n",
    "                    td_cells = row.find_all('td')\n",
    "                if len(td_cells) >= 2:\n",
    "                        a_tag = td_cells[1].find('a')\n",
    "                        if a_tag:\n",
    "                            citations.append(td_cells[0].get_text().strip())\n",
    "                            titles.append(a_tag.get_text().strip())\n",
    "                            hrefs.append(a_tag['href'])\n",
    "\n",
    "                # Create DataFrame\n",
    "                revoked_regs_data = pd.DataFrame({\n",
    "                    'revoked_reg_a_href': hrefs,\n",
    "                    'revoked_reg_citation': citations,\n",
    "                    'revoked_reg_title': titles\n",
    "                })\n",
    "            else:\n",
    "                print(\"No Revoked Regs (RR) rows found in the table.\")\n",
    "                pass\n",
    "        else:\n",
    "            print(\"RR Table not found in the HTML.\")\n",
    "            pass\n",
    "    else:\n",
    "        print(\"Revoked Regulations (RR) not found for \" + url + \".\")\n",
    "        pass\n",
    "\n",
    "        # Convert DataFrame to JSON object\n",
    "        revoked_regs_dict = revoked_regs_data.to_dict(orient='records')\n",
    "\n",
    "        return revoked_regs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract current regs\n",
    "def extract_current_regs_data(soup, url):\n",
    "    \"\"\"\n",
    "    Extracts data about current regulations from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): BeautifulSoup object of the page.\n",
    "        url (str): The URL of the web page being processed, used for logging and tracking purposes.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with data of current regulations.\n",
    "\n",
    "    #Example Usage\n",
    "    #url = \"https://www.ontario.ca/laws/statute/23s01\"\n",
    "    #current_regulations_data = extract_current_regs_data(url)\n",
    "\n",
    "    # Print first few JSON objects for demonstration\n",
    "    #for json_obj in current_regulations_data[:5]:\n",
    "    #   print(json_obj)\n",
    "    \"\"\"\n",
    "    # Initialize DataFrame\n",
    "    regulations_data = pd.DataFrame()\n",
    "\n",
    "    # Find the 'div' tag with the class \"versions\"\n",
    "    regulations = soup.find('div', id=\"regulations\")\n",
    "\n",
    "    if regulations is not None:\n",
    "        # Find the table\n",
    "        regulations_table = regulations.find('table', class_='act-reg-list noStripes')\n",
    "\n",
    "        if regulations_table:\n",
    "            regulations_rows = regulations_table.findAll('tr')\n",
    "\n",
    "            if regulations_rows:\n",
    "                # Initialize lists to store the data\n",
    "                citations, titles, hrefs = [], [], []\n",
    "\n",
    "                # Iterate over each row in the table\n",
    "                for row in regulations_rows:\n",
    "                    td_cells = row.find_all('td')\n",
    "                    if len(td_cells) >= 2:\n",
    "                        a_tag = td_cells[1].find('a')\n",
    "                        if a_tag:\n",
    "                            citations.append(td_cells[0].get_text().strip())\n",
    "                            titles.append(a_tag.get_text().strip())\n",
    "                            hrefs.append(a_tag['href'])\n",
    "\n",
    "                # Create DataFrame\n",
    "                regulations_data = pd.DataFrame({\n",
    "                    'a_href': hrefs,\n",
    "                    'Citation': citations,\n",
    "                    'title': titles\n",
    "                })\n",
    "            else:\n",
    "                print(\"No CR rows found in the table.\")\n",
    "                pass\n",
    "        else:\n",
    "            print(\"CR Table not found in the HTML.\")\n",
    "            pass\n",
    "    else:\n",
    "        print(\"Current Regulations (CR) tag not found for \" + url + \".\")\n",
    "        pass\n",
    "\n",
    "    # Convert DataFrame to JSON object\n",
    "    current_regs_dict = regulations_data.to_dict(orient='records')\n",
    "\n",
    "    return current_regs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get Act Info\n",
    "def parse_act_info(soup, url):\n",
    "    \"\"\"\n",
    "    Parses act information from the given BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): BeautifulSoup object containing the act page content.\n",
    "\turl (Str): url to fill in url field\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted act information.\n",
    "\n",
    "    # Example usage\n",
    "    #url = \"https://www.ontario.ca/laws/statute/90p10\"\n",
    "    #act_metadata_json_objects = parse_act_info(url)\n",
    "\n",
    "    # Print first few JSON objects for demonstration\n",
    "    #for json_obj in act_metadata_json_objects[:5]:\n",
    "    #    print(json_obj)\n",
    "    \"\"\"\n",
    "    # Get Short Title\n",
    "    current_li = soup.find('li', class_='current')\n",
    "    full_title = current_li.get_text() if current_li else 'Not Found'\n",
    "\n",
    "    # Get citation\n",
    "    act_name = soup.find('p', class_='shorttitle')\n",
    "    act_name_text = act_name.get_text() if act_name else 'Not Found'\n",
    "    citation_raw = full_title.replace(act_name_text, \"\") if full_title and act_name_text else ''\n",
    "    citation = citation_raw.lstrip(', ') if citation_raw else 'Not Found'\n",
    "\n",
    "    # Add Date Scraped Column\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Create dictionary of scraped data\n",
    "    act_info = {\n",
    "        \"full_title\": full_title,\n",
    "        \"act_name_text\": act_name_text,\n",
    "        \"citation\": citation,\n",
    "        \"url\": url,\n",
    "        \"date_scraped\": today\n",
    "    }\n",
    "\n",
    "    return act_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get Regs Info\n",
    "def parse_reg_info(soup, url):\n",
    "    \"\"\"\n",
    "    Parses regulation information from the given BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): BeautifulSoup object containing the regulation page content.\n",
    "\turl (Str): url to fill in url field\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted regulation information.\n",
    "\n",
    "    # Example usage\n",
    "    #url = \"https://www.ontario.ca/laws/regulation/100034\"\n",
    "    #reg_metadata_json_objects = scrape_reg_info(url)\n",
    "\n",
    "\n",
    "    # Print first few JSON objects for demonstration\n",
    "    #for json_obj in reg_metadata_json_objects[:5]:\n",
    "    #    print(json_obj)\n",
    "    \"\"\"\n",
    "    # Get Short Title\n",
    "    current_li = soup.find('li', class_='current')\n",
    "    full_title = current_li.get_text() if current_li else 'Not Found'\n",
    "\n",
    "    # Get Act Promulgated Under\n",
    "    act_under = soup.find('p', class_='shorttitle-e')\n",
    "    act_under_text = act_under.get_text() if act_under else 'Not Found'\n",
    "\n",
    "    # Get Reg name\n",
    "    reg_name = soup.find('p', class_='regtitle-e')\n",
    "    reg_name_text = reg_name.get_text() if reg_name else 'Not Found'\n",
    "\n",
    "    # Derive Citation\n",
    "    citation_raw = full_title.replace(reg_name_text, \"\") if full_title and reg_name_text else ''\n",
    "    citation = citation_raw.lstrip(': ') if citation_raw else 'Not Found'\n",
    "\n",
    "    # Add Date Scraped Column\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Create dictionary of scraped data\n",
    "    reg_info = {\n",
    "        \"full_title\": full_title,\n",
    "        \"reg_name_text\": reg_name_text,\n",
    "        \"citation\": citation,\n",
    "        \"act_under\": act_under_text,\n",
    "        \"url\": url,\n",
    "        \"date_scraped\": today\n",
    "    }\n",
    "\n",
    "    return reg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove trailing '.0'\n",
    "def remove_trailing_zero(number):\n",
    "    \"\"\"\n",
    "    Removes the trailing '.0' from a number represented as a string.\n",
    "\n",
    "    Args:\n",
    "    number (float or str): The number from which to remove the trailing '.0'.\n",
    "\n",
    "    Returns:\n",
    "    str: The number as a string without the trailing '.0', if present.\n",
    "    \"\"\"\n",
    "    number_str = str(number)\n",
    "    if number_str.endswith('.0'):\n",
    "        return number_str[:-1]  # Remove last two characters ('.0')\n",
    "    return number_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Process sections\n",
    "def process_section(section):\n",
    "    \"\"\"\n",
    "    Processes a given section of a legal document and returns the extracted content.\n",
    "\n",
    "    Args: \n",
    "        section (bs4.element.Tag): The section element to process.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the section's id, headnote, content, and raw HTML.\n",
    "    \"\"\"\n",
    "    #Initialize Data, current headnote\n",
    "    a_tag = section.find('a')\n",
    "    b_tag = section.find('b')\n",
    "\n",
    "    if a_tag is not None:\n",
    "        id_value = a_tag.get('name')\n",
    "    elif b_tag is not None:\n",
    "        id_value = b_tag.text.strip()\n",
    "    else:\n",
    "        id_value = None\n",
    "\n",
    "    # Capture the headnote for the current section\n",
    "    headnote_tag = section.find_previous('p', class_='headnote') or\\\n",
    "                     section.find_previous('p', class_='headnote-e') or\\\n",
    "                     section.find_previous('h3', class_='heading1')\n",
    "    current_headnote = headnote_tag.text if headnote_tag else None\n",
    "\n",
    "    content = [section.text]\n",
    "    raw_html = [str(section)]\n",
    "\n",
    "    for sibling in section.find_next_siblings():\n",
    "        #print(f\"Section: {current_headnote}, Sibling tag: {sibling.name}, class: {sibling.get('class', [])}\")\n",
    "\n",
    "        # Directly check if we've hit the start of a new section\n",
    "        if sibling.name == 'p' and 'headnote' in sibling.get('class', []) or 'headnote-e' in sibling.get('class', []):\n",
    "            # Look ahead to the next sibling\n",
    "            next_elem = sibling.find_next_sibling()\n",
    "            # Check if the next sibling is a section\n",
    "            if next_elem:\n",
    "                if ((next_elem.name == 'p' and 'section' in next_elem.get('class', [])) or \\\n",
    "                    (next_elem.name == 'p' and 'section-e' in next_elem.get('class', [])) or \\\n",
    "                 #   (next_elem.name == 'p' and 'headnote' in next_elem.get('class', [])) or \\ # experimental\n",
    "                 #   (next_elem.name == 'p' and 'headnote-e' in next_elem.get('class', [])) or \\ #experimental\n",
    "                 #   (next_elem.name == 'h2' and 'partnum' in next_elem.get('class', [])) or \\ #experimental \n",
    "                 #   (next_elem.name == 'h2' and 'partnum-e' in next_elem.get('class', [])) or \\ #experimental\n",
    "                    (next_elem.name == 'h3' and 'heading1' in next_elem.get('class', []))):\n",
    "                    break\n",
    "\n",
    "            # Skip if it's an amendments section\n",
    "            if sibling.name == 'p' and 'amendments-heading' in sibling.get('class', []):\n",
    "                continue\n",
    "        \n",
    "        sibling_text = sibling.text.replace('\\xa0', ' ').strip()\n",
    "        raw_html.append(str(sibling))\n",
    "        content.append(sibling_text)\n",
    "\n",
    "    # Return a dictionary for the current section\n",
    "    return {\n",
    "        'id': id_value,\n",
    "        'section': current_headnote,\n",
    "        'content': ' '.join(content),\n",
    "        'raw_html': ' '.join(raw_html)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get LAW from a traditional TOC-bearing page\n",
    "def scrape_TOC_law(url):\n",
    "    \"\"\"\n",
    "    Extracts and organizes legal information from a table of contents (TOC) on a given webpage.\n",
    "\n",
    "    This function processes a webpage containing legal documents, such as regulations or statutes, \n",
    "    represented by a traditional TOC structure. It scrapes and organizes the legal content \n",
    "    into a structured format.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): A BeautifulSoup object of the webpage, which is used for parsing HTML content.\n",
    "    url (str): The URL of the webpage to be scraped. This is used to retrieve tables with Pandas.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries where each dictionary contains detailed information about a section \n",
    "                or part of the legal document. The keys in the dictionary include 'ahref_id', 'TOCid', \n",
    "                'section', 'part_id', 'part_type', 'content', and 'raw_html', providing a comprehensive\n",
    "                overview of each section's content and metadata.\n",
    "\n",
    "    The function performs several steps:\n",
    "    1. Extracts tables and relevant links from the TOC.\n",
    "    2. Initializes and populates a DataFrame with the TOC information.\n",
    "    3. Adjusts TOC identifiers and retrieves corresponding content sections.\n",
    "    4. Merges TOC information with actual content, organizing it into a structured format.\n",
    "    5. Converts the organized data into a dictionary format suitable for JSON serialization.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    url = 'https://www.ontario.ca/laws/statute/90i03'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    legal_data = scrape_TOC_law(soup, url)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    #Create object page\n",
    "    webpage = requests.get(url)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "    \n",
    "    # Obtain info from TOC\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        if 'MsoNormalTable' in table.get('class', ''):\n",
    "            table1 = table\n",
    "            break\n",
    "    tocspans = table1.findAll('span')\n",
    "\n",
    "    # Initialize variables to store the previous part's TOCid and section\n",
    "    previous_part_tocid = None\n",
    "    previous_part_section = None  # Initialize previous_part_section\n",
    "\n",
    "    # Create a dictionary to store PARTS ahref pointers\n",
    "    parts_ahref_pointers = {}\n",
    "    for span in tocspans:\n",
    "        span_text = span.text\n",
    "        ahref = span.find('a', href=True)\n",
    "        if ahref:\n",
    "            parts_ahref_pointers[span_text] = ahref['href']\n",
    "\n",
    "    # Create dataframe to hold legislation info\n",
    "    leginfo = pd.DataFrame(columns=['ahref_id', 'TOCid', 'section'])\n",
    "\n",
    "    # Get Tables with Pandas\n",
    "    dfs = pd.read_html(url)\n",
    "\n",
    "    # Check for the presence of specific a href links\n",
    "    revoked_regulations_exists = bool(soup.find('a', href=\"#revoked_regulations\"))\n",
    "    current_regulations_exists = bool(soup.find('a', href=\"#regulations\"))\n",
    "\n",
    "    # Determine the TOC index based on the existence of specific links\n",
    "    if revoked_regulations_exists and current_regulations_exists:\n",
    "        TOC = dfs[3]  # Set to table at index 3\n",
    "    elif current_regulations_exists or revoked_regulations_exists:\n",
    "        TOC = dfs[2]  # Set to table at index 2\n",
    "    else:\n",
    "        TOC = dfs[1]  # Set to table at index 1\n",
    "\n",
    "    # Populate TOCid & section Title\n",
    "    leginfo['TOCid'] = TOC[0]\n",
    "    leginfo['section'] = TOC[1]\n",
    "\n",
    "    # Remove any trailing zeroes from tocid\n",
    "    leginfo['TOCid'] = leginfo['TOCid'].apply(remove_trailing_zero)\n",
    "\n",
    "    # Iterate through the 'leginfo' DataFrame and update ahref_id\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = row['TOCid'] or row['TOCid-e']\n",
    "        ahref = soup.find('a', string=TOCid, href=True)\n",
    "        if ahref:\n",
    "            leginfo.at[index, 'ahref_id'] = ahref['href']\n",
    "\n",
    "    # Update the 'leginfo' DataFrame with parts and part pointers\n",
    "    next_ahref_pointer = 0\n",
    "    for index, row in leginfo.iterrows():\n",
    "        ahref_id = row['ahref_id']\n",
    "        if pd.isna(ahref_id) and next_ahref_pointer < len(parts_ahref_pointers):\n",
    "            new_ahref_id = list(parts_ahref_pointers.values())[next_ahref_pointer]\n",
    "            TOCid = list(parts_ahref_pointers.keys())[next_ahref_pointer]\n",
    "            leginfo.at[index, 'ahref_id'] = new_ahref_id\n",
    "            leginfo.at[index, 'TOCid'] = TOCid\n",
    "            next_ahref_pointer += 1\n",
    "\n",
    "    # Update part_id and part_type in leginfo DataFrame\n",
    "    previous_part_tocid = None\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = str(row['TOCid'])\n",
    "        if TOCid.startswith(\"PART\"):\n",
    "            previous_part_tocid = TOCid\n",
    "            previous_part_section = row['section']\n",
    "        else:\n",
    "            leginfo.at[index, 'part_id'] = previous_part_tocid\n",
    "            leginfo.at[index, 'part_type'] = previous_part_section\n",
    "\n",
    "    full_data = []\n",
    "\n",
    "    # Your list of section classes\n",
    "    section_classes = ['section', 'section-e']\n",
    "\n",
    "    # Create a CSS selector string to select paragraphs with any of the classes\n",
    "    selector = ', '.join(f'p.{cls}' for cls in section_classes)\n",
    "\n",
    "    # Assuming 'soup' is your BeautifulSoup object\n",
    "    for section in soup.select('p.section') or soup.select('p.section-e'):\n",
    "        section_data = process_section(section)\n",
    "        full_data.append(section_data)\n",
    "\n",
    "    content_df = pd.DataFrame(full_data)\n",
    "    content_df['ahref_id'] = \"#\" + content_df['id']\n",
    "    content_df = content_df.drop(columns=['id'], axis=1)\n",
    "    content_df = content_df[['ahref_id', 'section', 'content', 'raw_html']]\n",
    "\n",
    "\n",
    "    # Merge leginfo with section content\n",
    "    leg_full = leginfo.merge(content_df, how=\"left\", on=['ahref_id'])\n",
    "\n",
    "\n",
    "\n",
    "    # Return the merged data\n",
    "    #return leg_full\n",
    "    # Convert DataFrame to dictionary for JSON use\n",
    "    return leg_full.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get LAW from a left-head-bearing page\n",
    "def scrape_lefthead_law(url):\n",
    "    \"\"\"\n",
    "    Extracts and organizes legal information from a webpage with a left-head-bearing layout.\n",
    "\n",
    "    This function is designed to scrape and structure legal content from webpages where the table of contents (TOC) \n",
    "    and legal sections are formatted with a left-head-bearing layout. It parses the webpage content and structures it \n",
    "    into a readable and usable format.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): A BeautifulSoup object for parsing HTML content of the webpage.\n",
    "    url (str): The URL of the webpage to be scraped. This is used to retrieve tables using Pandas.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries where each dictionary contains information about a legal section or part.\n",
    "                The keys in the dictionary include 'ahref_id', 'TOCid', 'section1', 'section2', 'part_id', and \n",
    "                corresponding section content and raw HTML.\n",
    "\n",
    "    The function performs several steps:\n",
    "    1. Extracts the TOC and identifies relevant links and parts pointers.\n",
    "    2. Initializes and populates a DataFrame with TOC and section information.\n",
    "    3. Associates each TOC entry with its corresponding section content.\n",
    "    4. Processes and collects content of each section based on predefined classes.\n",
    "    5. Merges the TOC information with the section content.\n",
    "    6. Converts the final structured data into a dictionary format suitable for JSON serialization.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    url = 'http://example.com/legal-doc'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    legal_data = scrape_lefthead_law(soup, url)\n",
    "    ```\n",
    "\n",
    "    Note:\n",
    "    This function is specifically tailored for webpages with a certain structure and may not work \n",
    "    correctly with pages of different formats.\n",
    "    \"\"\"\n",
    "    #Create object page\n",
    "    webpage = requests.get(url)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "    \n",
    "    # Process TOC spans and parts ahref pointers\n",
    "    table1 = soup.find('table', class_='MsoNormalTable') or soup.find('table', class_='MsoNormalTable-e')\n",
    "    tocspans = table1.findAll('span')\n",
    "    parts_ahref_pointers = {}\n",
    "    for span in tocspans:\n",
    "        span_text = span.text\n",
    "        ahref = span.find('a', href=True)\n",
    "        if ahref:\n",
    "            parts_ahref_pointers[span_text] = ahref['href']\n",
    "\n",
    "    # Create leginfo DataFrame\n",
    "    leginfo = pd.DataFrame(columns=['ahref_id', 'TOCid', 'section'])\n",
    "    dfs = pd.read_html(url)\n",
    "    TOC = dfs[1]\n",
    "    leginfo['TOCid'] = TOC[0]\n",
    "    leginfo['section1'] = TOC[1]\n",
    "    leginfo['section2'] = TOC[2]\n",
    "\n",
    "    # Update leginfo DataFrame with ahref ids\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = row['section1']\n",
    "        ahref = soup.find('a', string=TOCid, href=True) or soup.find('a', string=f\"{TOCid}-e\", href=True)\n",
    "        if ahref:\n",
    "            leginfo.at[index, 'ahref_id'] = ahref['href']\n",
    "\n",
    "    # Update leginfo DataFrame with parts and part pointers\n",
    "    next_ahref_pointer = 0\n",
    "    for index, row in leginfo.iterrows():\n",
    "        ahref_id = row['ahref_id']\n",
    "        if pd.isna(ahref_id) and next_ahref_pointer < len(parts_ahref_pointers):\n",
    "            new_ahref_id = list(parts_ahref_pointers.values())[next_ahref_pointer]\n",
    "            TOCid = list(parts_ahref_pointers.keys())[next_ahref_pointer]\n",
    "            leginfo.at[index, 'ahref_id'] = new_ahref_id\n",
    "            leginfo.at[index, 'TOCid'] = TOCid\n",
    "            next_ahref_pointer += 1\n",
    "\n",
    "    leginfo = leginfo.fillna('None')\n",
    "\n",
    "    # Update part_id and part_type in leginfo DataFrame\n",
    "    previous_part_tocid = None\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = row['TOCid']\n",
    "        if TOCid.startswith(\"PART\"):\n",
    "            previous_part_tocid = TOCid\n",
    "            previous_part_section = row['section']\n",
    "        else:\n",
    "            leginfo.at[index, 'part_id'] = previous_part_tocid\n",
    "\n",
    "    # Process sections and definitions\n",
    "    data = []\n",
    "\n",
    "    # Your list of section classes\n",
    "    section_classes = ['section', 'section-e']\n",
    "\n",
    "    # Create a CSS selector string to select paragraphs with any of the classes\n",
    "    selector = ', '.join(f'p.{cls}' for cls in section_classes)\n",
    "\n",
    "    # Assuming 'soup' is your BeautifulSoup object\n",
    "    for section in soup.select(selector):\n",
    "        section_data = process_section(section)\n",
    "        data.append(section_data)\n",
    "\n",
    "    content_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    # Merge leginfo with section content\n",
    "    leg_full = leginfo.merge(content_df, how=\"outer\", on=['ahref_id'])\n",
    "\n",
    "    # Convert DataFrame to dictionary for JSON use\n",
    "    return leg_full.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get LAW from a TOCless page\n",
    "def scrape_noTOC_law(url):\n",
    "    \"\"\"\n",
    "    Extracts and organizes legal information from a webpage that lacks a traditional table of contents (TOC).\n",
    "\n",
    "    This function is tailored to handle webpages where legal documents are presented without a clear TOC. It \n",
    "    scrapes the webpage content, particularly focusing on sections and definitions, and organizes it into \n",
    "    a structured format.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the webpage to be scraped.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, where each dictionary contains information about a section or definition\n",
    "                from the legal document. The structure of each dictionary includes keys like 'ahref_id', 'TOCid', \n",
    "                'section', and the content of each section.\n",
    "\n",
    "    The function works through these steps:\n",
    "    1. Initializes a DataFrame to hold legislation information.\n",
    "    2. Finds and processes all paragraphs classified as 'section' or 'definition'.\n",
    "    3. Creates a CSS selector to identify the relevant paragraphs.\n",
    "    4. Processes each identified section and collects its data.\n",
    "    5. Converts the collected data into a dictionary format suitable for JSON serialization.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    url = \"https://www.ontario.ca/laws/statute/23s01\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    legal_data = scrape_noTOC_law(soup, url)\n",
    "    ```\n",
    "\n",
    "    Note:\n",
    "    This function is specifically designed for webpages that do not have a TOC, and it relies on the presence \n",
    "    of specific HTML classes ('section', 'section-e', 'definition', 'definition-e') to identify relevant content.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create object page\n",
    "    webpage = requests.get(url)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "        \n",
    "    # Create dataframe to hold legislation info\n",
    "    leginfo = pd.DataFrame(columns=['ahref_id', 'TOCid', 'section'])\n",
    "\n",
    "    # Obtain sections and definitions\n",
    "    definitions = soup.findAll('p', class_='definition') or soup.findAll('p', class_='definition-e')\n",
    "    sections = soup.findAll('p', class_='section') or soup.findAll('p', class_='section-e')\n",
    "    all_elements = soup.findAll('p')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Your list of section classes\n",
    "    section_classes = ['section', 'section-e']\n",
    "\n",
    "    # Create a CSS selector string to select paragraphs with any of the classes\n",
    "    selector = ', '.join(f'p.{cls}' for cls in section_classes)\n",
    "\n",
    "    # Assuming 'soup' is your BeautifulSoup object\n",
    "    for section in soup.select(selector):\n",
    "        section_data = process_section(section)\n",
    "        data.append(section_data)\n",
    "\n",
    "    content_df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert DataFrame to dictionary for JSON use\n",
    "    return content_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get REGS from a traditional TOC-bearing page\n",
    "def scrape_TOC_reg(url):\n",
    "    \"\"\"\n",
    "    Extracts and organizes regulation information from a webpage with a traditional table of contents (TOC).\n",
    "\n",
    "    This function is designed to scrape and structure regulatory content from webpages where the content is \n",
    "    organized using a traditional table of contents (TOC). It extracts sections, parts, and their corresponding \n",
    "    content and organizes it into a structured format.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the webpage to be scraped.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, where each dictionary contains information about a section, part, or \n",
    "                definition from the regulatory document. The structure of each dictionary includes keys like \n",
    "                'ahref_id', 'TOCid', 'part_id', 'part_type', 'section', 'content', and 'raw_html'.\n",
    "\n",
    "    The function performs several steps:\n",
    "    1. Extracts information from the table of contents (TOC), including links and their descriptions.\n",
    "    2. Initializes and populates a DataFrame with TOC information.\n",
    "    3. Associates each TOC entry with its corresponding content section.\n",
    "    4. Processes and collects content of each section based on predefined classes.\n",
    "    5. Merges the TOC information with the section content.\n",
    "    6. Reorganizes and renames DataFrame columns for consistency.\n",
    "    7. Converts the final structured data into a dictionary format suitable for JSON serialization.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    url = 'https://www.ontario.ca/laws/regulation/070407'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    regulatory_data = scrape_TOC_reg(soup, url)\n",
    "    ```\n",
    "\n",
    "    Note:\n",
    "    This function assumes a specific format of the webpage with a table of contents (TOC) and specific HTML classes \n",
    "    ('section', 'section-e', 'definition', 'definition-e') for identifying relevant content.\n",
    "    \"\"\"\n",
    "    #Create object page\n",
    "    webpage = requests.get(url)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "    \n",
    "    #Obtain info from TOC\n",
    "    table1 = soup.find('table', class_ = 'MsoNormalTable') or soup.find('table', class_ = 'MsoNormalTable-e')\n",
    "    tocspans = table1.findAll('span')\n",
    "\n",
    "    # Create an empty dictionary to store PARTS ahref pointers\n",
    "    parts_ahref_pointers = {}\n",
    "\n",
    "    for span in tocspans:\n",
    "        # Extract the span text and the ahref attribute\n",
    "        span_text = span.text\n",
    "        ahref = span.find('a', href=True)\n",
    "\n",
    "        if ahref:\n",
    "            parts_ahref_pointers[span_text] = ahref['href']\n",
    "\n",
    "    #Create dataframe to hold leginfoislation\n",
    "    leginfo = pd.DataFrame(columns = ['ahref_id',\n",
    "                                'TOCid',\n",
    "                                'section'])\n",
    "\n",
    "    #Get Tables w Pandas\n",
    "    dfs = pd.read_html(url)\n",
    "    TOC = dfs[1]\n",
    "\n",
    "    #Populate TOCid & section Title\n",
    "    leginfo['TOCid'] = TOC[0]\n",
    "    leginfo['section1'] = TOC[1]\n",
    "    if 2 < len(TOC.columns):\n",
    "        leginfo['section2'] = TOC[2]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Iterate through the 'leginfo' DataFrame and fetch the corresponding <a> elements\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = row['TOCid']\n",
    "\n",
    "        # Find the <a> (ahref) element that matches the 'TOCid' in the DataFrame\n",
    "        ahref = soup.find('a', string=TOCid, href=True)\n",
    "\n",
    "        if ahref:\n",
    "            # Extract the 'href' attribute and assign it to 'ahref_id' column\n",
    "            leginfo.at[index, 'ahref_id'] = ahref['href']\n",
    "        else:\n",
    "            # Handle the case where no <a> (ahref) element is found for the 'TOCid'\n",
    "            #print(f\"No ahref found for TOCid: {TOCid}\")\n",
    "            pass\n",
    "\n",
    "\n",
    "    # Update the 'leginfo' DataFrame with parts and part pointers\n",
    "    next_ahref_pointer = 0  # Initialize the counter for sequential ahref pointers\n",
    "\n",
    "    for index, row in leginfo.iterrows():\n",
    "        ahref_id = row['ahref_id']\n",
    "\n",
    "        # Check if 'ahref_id' is NaN\n",
    "        if pd.isna(ahref_id):\n",
    "            if next_ahref_pointer < len(parts_ahref_pointers):\n",
    "                # Get the next 'ahref' pointer and 'TOCid' from the dictionary\n",
    "                new_ahref_id = list(parts_ahref_pointers.values())[next_ahref_pointer]\n",
    "                TOCid = list(parts_ahref_pointers.keys())[next_ahref_pointer]\n",
    "\n",
    "                # Assign the 'ahref' pointer and 'TOCid' to the respective columns\n",
    "                leginfo.at[index, 'ahref_id'] = new_ahref_id\n",
    "                leginfo.at[index, 'TOCid'] = TOCid\n",
    "\n",
    "                next_ahref_pointer += 1\n",
    "\n",
    "    leginfo = leginfo.fillna('None')\n",
    "\n",
    "    # Initialize variables to store the previous part's TOCid and section\n",
    "    previous_part_tocid = None\n",
    "    previous_part_section = None\n",
    "\n",
    "    # Iterate through the DataFrame and update part_id and part_type\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = row['TOCid']\n",
    "        section = row['section1']\n",
    "\n",
    "        if TOCid.startswith(\"PART\"):\n",
    "            # For Parts, update the previous_part_tocid and previous_part_section\n",
    "            previous_part_tocid = TOCid\n",
    "            previous_part_section = section\n",
    "        else:\n",
    "            # For sections, set part_id to the previous part's TOCid\n",
    "            leginfo.at[index, 'part_id'] = previous_part_tocid\n",
    "            # Set part_type to the previous part's section\n",
    "            leginfo.at[index, 'part_type'] = previous_part_section\n",
    "\n",
    "    #Obtain secs and defs\n",
    "    definitions = soup.findAll('p', class_ = 'definition-e') or soup.findAll('p', class_ = 'definition')\n",
    "    sections = soup.findAll('p', class_ = 'section-e') or soup.findAll('p', class_ = 'section')\n",
    "    all = soup.findAll('p')\n",
    "\n",
    "    full_data = []\n",
    "\n",
    "    # Assuming 'soup' is your BeautifulSoup object\n",
    "    for section in soup.select('p.section') or soup.select('p.section-e'):\n",
    "        section_data = process_section(section)\n",
    "        full_data.append(section_data)\n",
    "\n",
    "    content_df = pd.DataFrame(full_data)\n",
    "    content_df['ahref_id'] = \"#\" + content_df['id']\n",
    "    content_df = content_df.drop(columns=['id'], axis=1)\n",
    "    content_df = content_df[['ahref_id', 'section', 'content', 'raw_html']]\n",
    "\n",
    "    # Merge leginfo with section content\n",
    "    leg_full = leginfo.merge(content_df, how=\"left\", on=['ahref_id'])\n",
    "\n",
    "    # Optionally, you can rename section_y back to section\n",
    "    leg_full = leg_full.rename(columns={'section_y': 'section'})\n",
    "    \n",
    "    #Rename section 1 to section\n",
    "    leg_full['section'] = leg_full['section1']\n",
    "    leg_full = leg_full.drop(['section1'], axis = 1)\n",
    "\n",
    "    #Drop section 2 if Matching section\n",
    "    if 'section2' in leg_full.columns:\n",
    "        if (leg_full['section2'] == leg_full['section']).any():\n",
    "            leg_full = leg_full.drop(['section2'], axis=1)\n",
    "\n",
    "    #Reorder\n",
    "    leg_full = leg_full[['ahref_id', 'TOCid', 'part_id', 'part_type', 'section', 'content', 'raw_html']]\n",
    "\n",
    "    # Convert DataFrame to dictionary for JSON use\n",
    "    return leg_full.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get REGS from a TOCless page\n",
    "def scrape_noTOC_reg(url):\n",
    "    \"\"\"\n",
    "    Extracts regulatory information from a webpage without a traditional table of contents (TOC).\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the webpage to be scraped.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries containing information about regulatory sections and definitions \n",
    "                from the webpage.\n",
    "\n",
    "    The function extracts content from sections and definitions on the webpage and structures it into a \n",
    "    dictionary format suitable for JSON use.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    url = 'https://www.ontario.ca/laws/regulation/230267'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    regulatory_data = scrape_noTOC_reg(soup, url)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Create object page\n",
    "    webpage = requests.get(url)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "    # Create dataframe to hold legislation info\n",
    "    leginfo = pd.DataFrame(columns=['ahref_id', 'TOCid', 'Section'])\n",
    "\n",
    "    # Obtain sections and definitions\n",
    "    definitions = soup.findAll('p', class_='definition-e')\n",
    "    sections = soup.findAll('p', class_='section-e')\n",
    "    all_elements = soup.findAll('p')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    current_headnote = None  # Variable to store the current headnote\n",
    "\n",
    "    for section in soup.select('p.section-e'):\n",
    "        a_tag = section.find('a')\n",
    "        b_tag = section.find('b')\n",
    "\n",
    "        if a_tag is not None:\n",
    "            id_value = a_tag.get('name')\n",
    "        elif b_tag is not None:\n",
    "            id_value = b_tag.text.strip()\n",
    "        else:\n",
    "            id_value = None\n",
    "\n",
    "        # Capture the headnote for the current section\n",
    "        headnote_tag = section.find_previous('p', class_='headnote-e') or\\\n",
    "                        section.find_previous('p', class_='headnote') or \\\n",
    "                        section.find_previous('p', class_='heading1') or \\\n",
    "                        section.find_previous('p', class_='heading1-e')\n",
    "        if headnote_tag:\n",
    "            current_headnote = headnote_tag.text\n",
    "\n",
    "        content = [section.text]\n",
    "        raw_html = [str(section)]\n",
    "        \n",
    "        for sibling in section.find_next_siblings():\n",
    "            # Break the loop if the next section or a headnote is encountered\n",
    "            if sibling.name == 'p' and ('section-e' in sibling.get('class', []) or 'headnote-e' in sibling.get('class', [])) or \\\n",
    "                                        ('section' in sibling.get('class', []) or 'headnote' in sibling.get('class', [])):\n",
    "                break\n",
    "\n",
    "            sibling_text = sibling.text.replace('\\xa0', ' ').strip()\n",
    "\n",
    "            raw_html.append(str(sibling))\n",
    "\n",
    "            if sibling.name == 'p' or sibling.name == 's':\n",
    "                content.append(sibling_text)\n",
    "            else:\n",
    "                content.append(sibling_text)\n",
    "\n",
    "        data.append({'id': id_value, 'section': current_headnote, 'content': ' '.join(content), 'raw_html': ' '.join(raw_html)})\n",
    "\n",
    "    content_df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert DataFrame to dictionary for JSON use\n",
    "    return content_df.to_dict(orient='records') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get REGS from a left-head page\n",
    "def scrape_regs_lefthead(url):\n",
    "    \"\"\"\n",
    "    Extracts regulations from a webpage with a left-head table of contents (TOC).\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the webpage to be scraped.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries containing structured regulatory information, including sections,\n",
    "                parts, and content from the webpage.\n",
    "\n",
    "    The function extracts content from sections, parts, and definitions on the webpage and structures it \n",
    "    into a dictionary format suitable for JSON use.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    url = 'https://www.ontario.ca/laws/regulation/900664'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    regulatory_data = scrape_regs_lefthead(soup, url)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Fetch and parse the webpage\n",
    "    webpage = requests.get(url)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "    # Extracting TOC and parts pointers\n",
    "    parts_ahref_pointers = {}\n",
    "    table1 = soup.find('table', class_='MsoNormalTable')\n",
    "    tocspans = table1.findAll('span')\n",
    "    for span in tocspans:\n",
    "        span_text = span.text\n",
    "        ahref = span.find('a', href=True)\n",
    "        if ahref:\n",
    "            parts_ahref_pointers[span_text] = ahref['href']\n",
    "\n",
    "    # Create dataframe to hold legislation information\n",
    "    leginfo = pd.DataFrame(columns=['ahref_id', 'TOCid', 'Section'])\n",
    "\n",
    "    # Extract tables with Pandas and populate the DataFrame\n",
    "    dfs = pd.read_html(url)\n",
    "    TOC = dfs[1]\n",
    "    leginfo['TOCid'] = TOC[0]\n",
    "    leginfo['Section1'] = TOC[1]\n",
    "    leginfo['Section2'] = TOC[2]\n",
    "\n",
    "    # Iterate through the 'leginfo' DataFrame and fetch the corresponding <a> elements\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = row['Section1']\n",
    "\n",
    "        # Find the <a> (ahref) element that matches the 'TOCid' in the DataFrame\n",
    "        ahref = soup.find('a', string=TOCid, href=True)\n",
    "\n",
    "        if ahref:\n",
    "            # Extract the 'href' attribute and assign it to 'ahref_id' column\n",
    "            leginfo.at[index, 'ahref_id'] = ahref['href']\n",
    "        else:\n",
    "            # Handle the case where no <a> (ahref) element is found for the 'TOCid'\n",
    "            # print(f\"No ahref found for TOCid: {TOCid}\")\n",
    "            pass\n",
    "\n",
    "\n",
    "    # Update the 'leginfo' DataFrame with parts and part pointers\n",
    "    next_ahref_pointer = 0  # Initialize the counter for sequential ahref pointers\n",
    "\n",
    "    for index, row in leginfo.iterrows():\n",
    "        ahref_id = row['ahref_id']\n",
    "\n",
    "        # Check if 'ahref_id' is NaN\n",
    "        if pd.isna(ahref_id):\n",
    "            if next_ahref_pointer < len(parts_ahref_pointers):\n",
    "                # Get the next 'ahref' pointer and 'TOCid' from the dictionary\n",
    "                new_ahref_id = list(parts_ahref_pointers.values())[next_ahref_pointer]\n",
    "                TOCid = list(parts_ahref_pointers.keys())[next_ahref_pointer]\n",
    "\n",
    "                # Assign the 'ahref' pointer and 'TOCid' to the respective columns\n",
    "                leginfo.at[index, 'ahref_id'] = new_ahref_id\n",
    "                leginfo.at[index, 'TOCid'] = TOCid\n",
    "\n",
    "                next_ahref_pointer += 1\n",
    "\n",
    "    #Fill NaNs\n",
    "    leginfo = leginfo.fillna('None')\n",
    "\n",
    "    # Initialize variables to store the previous part's TOCid and Section\n",
    "    previous_part_tocid = None\n",
    "    previous_part_section = None\n",
    "\n",
    "    # Iterate through the DataFrame and update part_id and part_type\n",
    "    for index, row in leginfo.iterrows():\n",
    "        TOCid = row['TOCid']\n",
    "        section = row['Section']\n",
    "\n",
    "        if TOCid.startswith(\"PART\"):\n",
    "            # For Parts, update the previous_part_tocid and previous_part_section\n",
    "            previous_part_tocid = TOCid\n",
    "            previous_part_section = section\n",
    "        else:\n",
    "            # For Sections, set part_id to the previous part's TOCid\n",
    "            leginfo.at[index, 'part_id'] = previous_part_tocid\n",
    "            # Set part_type to the previous part's section\n",
    "            leginfo.at[index, 'part_type'] = previous_part_section\n",
    "\n",
    "    #Obtain secs and defs\n",
    "    definitions = soup.findAll('p', class_ = 'definition-e')\n",
    "    sections = soup.findAll('p', class_ = 'section-e') or soup.findAll('p', class_ = 'section')\n",
    "    all = soup.findAll('p')\n",
    "\n",
    "\n",
    "    data = []\n",
    "\n",
    "    current_headnote = None  # Variable to store the current headnote\n",
    "\n",
    "    for section in soup.select('p.section-e') or soup.select('p.section'):\n",
    "        a_tag = section.find('a')\n",
    "        b_tag = section.find('b')\n",
    "\n",
    "        if a_tag is not None:\n",
    "            id_value = a_tag.get('name')\n",
    "        elif b_tag is not None:\n",
    "            id_value = b_tag.text.strip()\n",
    "        else:\n",
    "            id_value = None\n",
    "\n",
    "        # Capture the headnote for the current section\n",
    "        headnote_tag = section.find_previous('p', class_='headnote-e') or section.find_previous('p', class_='headnote') or section.find_previous('p', class_='heading1')\n",
    "        if headnote_tag:\n",
    "            current_headnote = headnote_tag.text\n",
    "\n",
    "        content = [section.text]\n",
    "        raw_html = [str(section)]\n",
    "        \n",
    "        for sibling in section.find_next_siblings():\n",
    "            # Break the loop if the next section or a headnote is encountered\n",
    "            if sibling.name == 'p' and ('section-e' in sibling.get('class', []) or 'headnote-e' in sibling.get('class', [])):\n",
    "                break\n",
    "\n",
    "            sibling_text = sibling.text.replace('\\xa0', ' ').strip()\n",
    "\n",
    "            raw_html.append(str(sibling))\n",
    "\n",
    "            if sibling.name == 'p' or sibling.name == 's':\n",
    "                content.append(sibling_text)\n",
    "            else:\n",
    "                content.append(sibling_text)\n",
    "\n",
    "        data.append({'id': id_value, 'Section': current_headnote, 'content': ' '.join(content), 'raw_html': ' '.join(raw_html)})\n",
    "\n",
    "    #Make df\n",
    "    content_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    #Rename a name column to match leginfo df,\n",
    "    content_df['ahref_id'] = (\"#\" + content_df['id'])\n",
    "\n",
    "\n",
    "    #Drop old a name and Section\n",
    "    content_df = content_df.drop(['id'], axis=1)\n",
    "\n",
    "    #Drop Section\n",
    "    content_df = content_df.drop(['Section'], axis=1)\n",
    "\n",
    "    #Reorder\n",
    "    content_df = content_df[['ahref_id', 'content', 'raw_html']]\n",
    "\n",
    "\n",
    "    #Merge leginfo with section content\n",
    "    leg_full = leginfo.merge(content_df, how=\"outer\", on=['ahref_id'])\n",
    "   \n",
    "\n",
    "    # Convert the final DataFrame to JSON\n",
    "    lefthead_reg_dict = leg_full.to_dict(orient='records')\n",
    "\n",
    "    return lefthead_reg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get versions\n",
    "def scrape_versions(driver, url):\n",
    "    \"\"\"\n",
    "    Extracts version information from a law or reg.\n",
    "\n",
    "    Args:\n",
    "    driver: WebDriver object for web page control.\n",
    "    soup (BeautifulSoup): Parsed HTML content of the webpage.\n",
    "    url (str): URL of the webpage.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: List of version data (href, valid_from, valid_to) in dictionaries.\n",
    "\n",
    "    This function loads more versions if available, extracts version data from a table,\n",
    "    and returns it in a structured dictionary format for JSON use.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    driver = webdriver.Chrome()\n",
    "    url = 'https://www.ontario.ca/laws/regulation/210679'\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    version_data = scrape_versions(driver, url)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    #Create object page\n",
    "    webpage = requests.get(url)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "    \n",
    "    try:\n",
    "        # Check for the presence of the 'More Versions' link\n",
    "        try:\n",
    "            link = driver.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tr.more-versions.hide a\")))\n",
    "            link.click()\n",
    "        except:\n",
    "            pass  # Suppressing print statements\n",
    "\n",
    "        # Find the 'div' tag with the class \"versions\"\n",
    "        versions = soup.find('div', id=\"versions\")\n",
    "\n",
    "        # Find the table\n",
    "        versions_table = soup.find('table', class_='act-reg-list noStripes')\n",
    "\n",
    "        # Get versions rows\n",
    "        version_rows = versions_table.findAll('tr') if versions_table else []\n",
    "\n",
    "        # Initialize lists to store the data\n",
    "        hrefs = []\n",
    "        valid_froms = []\n",
    "        valid_tos = []\n",
    "\n",
    "        # Iterate over each row in the table\n",
    "        for row in version_rows:\n",
    "            td_cells = row.find_all('td')\n",
    "            if len(td_cells) >= 2:\n",
    "                a_tag = td_cells[1].find('a')\n",
    "                if a_tag:\n",
    "                    span_tags = td_cells[1].find_all('span', class_='time')\n",
    "\n",
    "                    # Extract the href attribute\n",
    "                    href = a_tag['href']\n",
    "\n",
    "                    # Extract valid_from and valid_to\n",
    "                    valid_from = span_tags[0].get_text().strip() if span_tags else 'N/A'\n",
    "                    valid_to = span_tags[1].get_text().strip() if len(span_tags) > 1 else 'current'\n",
    "\n",
    "                    # Append the data to the lists\n",
    "                    hrefs.append(href)\n",
    "                    valid_froms.append(valid_from)\n",
    "                    valid_tos.append(valid_to)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        versions_data = pd.DataFrame({\n",
    "            'a_href': hrefs,\n",
    "            'valid_from': valid_froms,\n",
    "            'valid_to': valid_tos\n",
    "        })\n",
    "\n",
    "        # Convert DataFrame to dictionary for JSON use\n",
    "        return versions_data.to_dict(orient='records')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright Blurb Function\n",
    "def create_copyright_entry():\n",
    "    \"\"\"\n",
    "    Creates a copyright entry dictionary.\n",
    "\n",
    "    Returns:\n",
    "    dict: Copyright entry with the year and copyright holder.\n",
    "\n",
    "    This function generates a dictionary containing copyright information.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    copyright_info = create_copyright_entry()\n",
    "    ```\n",
    "    \"\"\"\n",
    "    return {\"Copyright\": \"© King's Printer for Ontario, 2023.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to combine law data into dict\n",
    "def combine_law_data(driver, soup, url, content):\n",
    "    \"\"\"\n",
    "    Retrieves various pieces of law data from a given URL.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL from which to scrape the law data.\n",
    "    content (dict/json): A dictionary/list of objects containing the content of the act\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing combined law data.\n",
    "    \n",
    "    Example Usage:\n",
    "    '''\n",
    "        #url = 'https://www.ontario.ca/laws/statute/97o25a'\n",
    "        #content = scrape_TOC_law(soup, url)\n",
    "\n",
    "        #combined_data_df = combine_law_data(soup, url, content )\n",
    "\n",
    "        #print(combined_data_df)\n",
    "    '''\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Act Info\n",
    "    act_info_json = parse_act_info(soup, url)\n",
    "\n",
    "    #Get Current Regs, if available\n",
    "    try:\n",
    "        # Get Current Regs\n",
    "        current_regs_json = extract_current_regs_data(soup, url)\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        # Handle exceptions (log them, pass, or take other actions)\n",
    "        print(f\"Current Regs not found for: {e}\")\n",
    "        # Decide what\n",
    "\n",
    "    #Get Revoked Regs, if available\n",
    "    try:\n",
    "        # Attempt to extract revoked regulations\n",
    "        revoked_regs_json = extract_revoked_regs_data(soup, url)\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        # Handle exceptions (log them, pass, or take other actions)\n",
    "        print(f\"Revoked Regs not found for: {e}\")\n",
    "\n",
    "    # Get Versions\n",
    "    versions_json = scrape_versions(driver, url)\n",
    "\n",
    "    #Get (C)\n",
    "    copyright = create_copyright_entry()\n",
    "\n",
    "    # Combine all the data into a single dictionary\n",
    "    combined_data = {\n",
    "        \"act_info\": act_info_json,\n",
    "        \"copyright\": copyright,\n",
    "        \"versions\": versions_json\n",
    "    }\n",
    "\n",
    "    # Try to add current_regs if it exists\n",
    "    try:\n",
    "        combined_data[\"current_regs\"] = current_regs_json\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    # Try to add revoked_regs if it exists\n",
    "    try:\n",
    "        combined_data[\"revoked_regs\"] = revoked_regs_json\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    # Add content at the end\n",
    "    combined_data[\"content\"] = content\n",
    "\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Save Law Data to JSON\n",
    "def save_law_data(combined_data, act_info_json, valid_from, valid_to, db_folder=\"db\"):\n",
    "    \"\"\"\n",
    "    Saves the combined law data to a JSON file.\n",
    "\n",
    "    Args:\n",
    "    combined_data (dict): The combined law data to be saved.\n",
    "    act_info_json (dict): JSON data containing act information.\n",
    "    valid_from (str): The start date of the act's validity.\n",
    "    valid_to (str): The end date of the act's validity.\n",
    "    db_folder (str, optional): The directory to save the file. Defaults to \"db\".\n",
    "\n",
    "    \n",
    "    # Example usage\n",
    "    '''\n",
    "        #combined_data = scrape_TOC_law('https://www.ontario.ca/laws/statute/21b02')\n",
    "        #act_info_json = scrape_act_info('https://www.ontario.ca/laws/statute/21b02')\n",
    "        #copyright = create_copyright_entry()\n",
    "        #save_law_data(combined_data, act_info_json, \"zzz\", \"yyy\")\n",
    "    '''\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract information from the act_info_json\n",
    "    act_full_title = act_info_json.get('full_title', \"\")\n",
    "    act_name_text = act_info_json.get('act_name_text', \"\")\n",
    "    citation = act_info_json.get('citation', \"\")\n",
    "    act_url = act_info_json.get('url', \"\")\n",
    "    date_scraped = act_info_json.get('date_scraped', \"\")\n",
    "\n",
    "    # Ensure that the necessary data is available\n",
    "    if act_full_title and date_scraped:\n",
    "        # Generate file name using sanitized strings\n",
    "        sanitized_title = sanitize_filename(act_full_title)\n",
    "        sanitized_valid_from = sanitize_filename(valid_from)\n",
    "        sanitized_valid_to = sanitize_filename(valid_to)\n",
    "        sanitized_date_scraped = sanitize_filename(date_scraped)\n",
    "\n",
    "        file_name = f\"{sanitized_title} + {sanitized_valid_from} - {sanitized_valid_to} + {sanitized_date_scraped}.json\"\n",
    "\n",
    "        # Define the file path for the file\n",
    "        file_path = os.path.join(db_folder, file_name)\n",
    "\n",
    "        # Create db folder if it doesn't exist\n",
    "        os.makedirs(db_folder, exist_ok=True)\n",
    "\n",
    "        # Write the combined data to a file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(combined_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Combined JSON data for version saved in {file_path}\")\n",
    "    else:\n",
    "        # Fallback filename if combined_data is empty\n",
    "        file_name = \"default_filename.json\"\n",
    "        file_path = os.path.join(db_folder, file_name)\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(combined_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Default JSON data saved in {file_path}\")\n",
    "\n",
    "    # Return the last file path for further use, or modify as needed\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Save Versions Data\n",
    "def save_version_data(version_row, combined_data, db_folder):\n",
    "    \"\"\"\n",
    "    Saves version data to a JSON file.\n",
    "\n",
    "    Args:\n",
    "    version_row (dict): The version information containing 'a_href', 'valid_from', and 'valid_to'.\n",
    "    combined_data (dict): The data to be saved.\n",
    "    db_folder (str): The folder path where the data will be saved.\n",
    "\n",
    "    This function saves the provided data to a JSON file with a filename based on the version information.\n",
    "\n",
    "    The JSON file is saved in the specified 'db_folder' directory. If the directory doesn't exist, it will be created.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    version_info = {\n",
    "        'a_href': 'https://www.ontario.ca/laws/statute/97o25a',\n",
    "        'valid_from': '2023-01-01',\n",
    "        'valid_to': 'current'\n",
    "    }\n",
    "    data_to_save = {'key1': 'value1', 'key2': 'value2'}\n",
    "    db_folder_path = '/path/to/database/folder'\n",
    "\n",
    "    save_version_data(version_info, data_to_save, db_folder_path)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Extract version identifier from the 'a_href' column\n",
    "    version_identifier = version_row['a_href'].split('/')[-1]  # This gets the 'vXX' part\n",
    "    valid_from = sanitize_filename(version_row['valid_from'])\n",
    "    valid_to = sanitize_filename(version_row['valid_to'] if version_row['valid_to'] != 'current' else 'current')\n",
    "\n",
    "    # Create the filename\n",
    "    file_name = f\"{version_identifier}_{valid_from}_to_{valid_to}.json\"\n",
    "\n",
    "    # Check if the db folder exists, and if not, create it\n",
    "    if not os.path.exists(db_folder):\n",
    "        os.makedirs(db_folder)\n",
    "\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(db_folder, file_name)\n",
    "\n",
    "    # Write the combined data to a file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(combined_data, file, indent=4)\n",
    "\n",
    "    print(f\"Data for version {version_identifier} saved in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Law Tri-Split Scraping Function\n",
    "def scrape_law_page(driver, soup, url, valid_from, valid_to):\n",
    "    \"\"\"\n",
    "    Scrape law content from a web page and save it.\n",
    "\n",
    "    Args:\n",
    "    driver (Selenium webdriver): To load dynamic elements on page\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML.\n",
    "    url (str): The URL of the law page.\n",
    "    valid_from (str): The valid from date for the law.\n",
    "    valid_to (str): The valid to date for the law.\n",
    "\n",
    "    This function scrapes law content from a web page and saves it in the appropriate format based on the structure of the law page.\n",
    "\n",
    "    - If the page has no table of contents (TOCless), it uses 'scrape_noTOC_law'.\n",
    "    - If the page has a left-side TOC, it uses 'scrape_lefthead_law'.\n",
    "    - If the page has a regular TOC, it uses 'scrape_TOC_law'.\n",
    "    - If the page has an unexpected structure, it handles it accordingly.\n",
    "\n",
    "    The scraped data is combined into a dictionary and saved as a JSON file with a filename based on the law's metadata.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "     # Make a GET request to the law page and create a BeautifulSoup object\n",
    "    url = 'https://www.ontario.ca/laws/statute/90e09'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Define valid_from and valid_to dates\n",
    "    valid_from_date = '2023-01-01'\n",
    "    valid_to_date = 'current'\n",
    "\n",
    "    # Scrape and save law data\n",
    "    scrape_law_page(soup, url, valid_from_date, valid_to_date)\n",
    "    ```\n",
    "    \"\"\"\n",
    "#Find Toc, Get Filename Parameters\n",
    "    toc = soup.find('table', class_='MsoNormalTable')\n",
    "\n",
    "    #Scrape Act Info/MetaData for filename parameters\n",
    "    act_info_json = parse_act_info(soup, url)\n",
    "\n",
    "#If TOCless\n",
    "    if not toc:\n",
    "        print(\"Structure Type: No Law TOC (TOCless logic) for \" + str(url))\n",
    "        \n",
    "        # Get TOC-less Content\n",
    "        noTOC_content = scrape_noTOC_law(url)\n",
    "\n",
    "        #Combine into dict\n",
    "        combined_data = combine_law_data(driver, soup, url, noTOC_content)\n",
    "\n",
    "        #Save File\n",
    "        save_law_data(combined_data, act_info_json, valid_from, valid_to)\n",
    "        \n",
    "\n",
    "\n",
    "#If LeftHead\n",
    "    elif toc.find('p', class_='TOCheadLeft'):\n",
    "        print(\"Structure Type: Law Left-side TOC (left-side TOC logic)\")\n",
    "\n",
    "        #Get LeftHead Law Content\n",
    "        lefthead_content = scrape_lefthead_law(url)\n",
    "\n",
    "        #Combine into dict\n",
    "        combined_data = combine_law_data(driver, soup, url, lefthead_content)\n",
    "\n",
    "        #Save File\n",
    "        save_law_data(combined_data, act_info_json, valid_from, valid_to)\n",
    "\n",
    "\n",
    "#If Normal\n",
    "    elif toc.find('p', class_='table'):\n",
    "        print(\"Structure Type: Regular Law TOC (regular logic)\")\n",
    "\n",
    "        #Get LeftHead Law Content\n",
    "        TOC_law_content = scrape_TOC_law(url)\n",
    "\n",
    "        #Combine into dict\n",
    "        combined_data = combine_law_data(driver, soup, url, TOC_law_content )\n",
    "\n",
    "        #Save File\n",
    "        save_law_data(combined_data, act_info_json, valid_from, valid_to)\n",
    "\n",
    "#If Unexpected\n",
    "    else:\n",
    "        print(\"Structure Type: Unknown Law\" + \":\" + str(url))\n",
    "        # Handle any unexpected structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to combine reg data into dict\n",
    "def combine_reg_data(driver, soup, url, content):\n",
    "    \"\"\"\n",
    "    Retrieves various pieces of law data from a given URL.\n",
    "\n",
    "    Args:\n",
    "    driver (Selenium webdriver): To load dynamic elements on page\n",
    "    soup (BeautifulSoup object): BS object to pass to functions\n",
    "    url (str): The URL from which to scrape the law data.\n",
    "    content (dict/json): A dictionary/list of objects containing the content of the act\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing combined law data.\n",
    "    \"\"\"\n",
    "    #Get Reg Info\n",
    "    reg_info_json = parse_reg_info(soup, url)\n",
    "\n",
    "    #Get Versions\n",
    "    versions_json = scrape_versions(driver, url)\n",
    "\n",
    "    #Get (C)\n",
    "    copyright = create_copyright_entry()\n",
    "\n",
    "    #Combine data into dict\n",
    "    combined_data = {\"reg_info\": reg_info_json,\n",
    "                    \"copyright\": copyright,\n",
    "                    \"versions\":versions_json,\n",
    "                    \"content\": content\n",
    "                    }\n",
    "\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Save Regulation Data to JSON\n",
    "def save_reg_data(combined_data, reg_info_json, valid_from, valid_to, db_folder=\"db\"):\n",
    "    \"\"\"\n",
    "    Saves the combined regulation data to a JSON file.\n",
    "\n",
    "    Args:\n",
    "    combined_data (dict): The combined regulation data to be saved.\n",
    "    reg_info_json (dict): Dictionary with regulation information.\n",
    "    valid_from (str): The start date of the regulation's validity.\n",
    "    valid_to (str): The end date of the regulation's validity.\n",
    "    db_folder (str, optional): The directory to save the file. Defaults to \"db\".\n",
    "\n",
    "    Example:\n",
    "    # Example usage\n",
    "    reg_info = parse_reg_info('https://www.ontario.ca/laws/regulation/100034')\n",
    "    copyright = create_copyright_entry()\n",
    "\n",
    "    # Accessing 'valid_from' and 'valid_to' dates\n",
    "    save_reg_data(copyright, reg_info, 'test1', 'test2')\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the db folder exists, and if not, create it\n",
    "    if not os.path.exists(db_folder):\n",
    "        os.makedirs(db_folder)\n",
    "\n",
    "    # Generate sanitized file name components\n",
    "    sanitized_valid_from = sanitize_filename(valid_from)\n",
    "    sanitized_valid_to = sanitize_filename(valid_to)\n",
    "\n",
    "    if reg_info_json:\n",
    "        # Initialize file name components with placeholders or actual values\n",
    "        file_name_components = {\n",
    "            'title': sanitize_filename(reg_info_json.get('full_title', '')),\n",
    "            'act_under': sanitize_filename(reg_info_json.get('act_under', '')),\n",
    "            'date_scraped': sanitize_filename(reg_info_json.get('date_scraped', ''))\n",
    "        }\n",
    "\n",
    "        # Generate file name by concatenating components with specific separators\n",
    "        file_name = (file_name_components['title'] + \" + \" +\n",
    " #            file_name_components['act_under'] + \" + \" +\n",
    "             sanitized_valid_from + \" - \" +\n",
    "             sanitized_valid_to + \" + \" +\n",
    "             file_name_components['date_scraped']) + \".json\"\n",
    "\n",
    "        file_path = os.path.join(db_folder, file_name)\n",
    "\n",
    "        # Write the combined data to a file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(combined_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Combined JSON data for version saved in {file_path}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback filename if reg_info_json is empty\n",
    "        file_name = \"default_filename.json\"\n",
    "        file_path = os.path.join(db_folder, file_name)\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(combined_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Default JSON data saved in {file_path}\")\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regulations Tri-Split Scraping Function\n",
    "def scrape_reg_page(driver, soup, url, valid_from, valid_to):\n",
    "    \"\"\"\n",
    "    Scrape regulation content from a web page and save it.\n",
    "\n",
    "    Args:\n",
    "    driver (Selenium webdriver): To load dynamic elements on page\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML.\n",
    "    url (str): The URL of the regulation page.\n",
    "    valid_from (str): The valid from date for the regulation.\n",
    "    valid_to (str): The valid to date for the regulation.\n",
    "\n",
    "    This function scrapes regulation content from a web page and saves it in the appropriate format based on the structure of the regulation page.\n",
    "\n",
    "    - If the page has no table of contents (TOCless), it uses 'scrape_noTOC_reg'.\n",
    "    - If the page has a left-side TOC, it uses 'scrape_regs_lefthead'.\n",
    "    - If the page has a regular TOC, it uses 'scrape_TOC_reg'.\n",
    "    - If the page has an unexpected structure, it handles it accordingly.\n",
    "\n",
    "    The scraped data is combined into a dictionary and saved as a JSON file with a filename based on the regulation's metadata.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "     # Make a GET request to the regulation page and create a BeautifulSoup object\n",
    "    url = 'https://www.ontario.ca/laws/regulation/940275'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Define valid_from and valid_to dates\n",
    "    valid_from_date = '2023-01-01'\n",
    "    valid_to_date = 'current'\n",
    "\n",
    "    # Scrape and save regulation data\n",
    "    scrape_reg_page(soup, url, valid_from_date, valid_to_date)\n",
    "    ```\n",
    "    \"\"\"\n",
    "#Get TOC and Filename Data\n",
    "    #Get TOC\n",
    "    toc = soup.find('table', class_='MsoNormalTable')\n",
    "\n",
    "    #Get Reg Info\n",
    "    reg_info_json = parse_reg_info(soup, url)\n",
    "\n",
    "#If TOCLess Reg\n",
    "    if not toc:\n",
    "        print(\"Structure Type: No Reg TOC (TOCless logic)\")\n",
    "\n",
    "        #GetTocLess Content\n",
    "        noTOC_reg_content = scrape_noTOC_reg(url)\n",
    "\n",
    "        #Combine into dict\n",
    "        combined_data = combine_reg_data(driver, soup, url, noTOC_reg_content)\n",
    "\n",
    "        #Save File\n",
    "        save_reg_data(combined_data, reg_info_json, valid_from, valid_to)\n",
    "\n",
    "#If LeftHead Reg\n",
    "    elif toc.find('p', class_='TOCheadLeft-e') or toc.find('p', class_='TOCpartLeft-e'):\n",
    "        print(\"Structure Type: Left-side Reg TOC (left-side TOC logic)\")\n",
    "\n",
    "        #Get LeftHead Content\n",
    "        lefthead_regs_content = scrape_regs_lefthead(url)\n",
    "\n",
    "        #Combine into dict\n",
    "        combined_data = combine_reg_data(driver, soup, url, lefthead_regs_content)\n",
    "\n",
    "        #Save File\n",
    "        save_reg_data(combined_data, reg_info_json, valid_from, valid_to)\n",
    "\n",
    "#If Normal\n",
    "    elif toc.find('p', class_='TOCid-e'):\n",
    "        print(\"Structure Type: Regular Reg TOC (regular logic)\")\n",
    "\n",
    "        #Get TOC-based Content\n",
    "        TOC_reg_content = scrape_TOC_reg(url)\n",
    "\n",
    "        #Combine into dict\n",
    "        combined_data = combine_reg_data(driver, soup, url, TOC_reg_content)\n",
    "\n",
    "        #Save File\n",
    "        save_reg_data(combined_data, reg_info_json, valid_from, valid_to)\n",
    "\n",
    "# If No-TOC, but with MsoNormal Table\n",
    "    elif toc and not toc.find('p', class_='TOCid-e'):\n",
    "        print(\"Structure Type: TOCless w/ Table Reg TOC\")\n",
    "        # Implement logic for this specific case here\n",
    "\n",
    "        #GetTocLess Content\n",
    "        noTOC_reg_content = scrape_noTOC_reg(url)\n",
    "\n",
    "        #Combine into dict\n",
    "        combined_data = combine_reg_data(driver, soup, url, noTOC_reg_content)\n",
    "\n",
    "        #Save File\n",
    "        save_reg_data(combined_data, reg_info_json, valid_from, valid_to)\n",
    "\n",
    "#If Unknown\n",
    "    else:\n",
    "        print(\"Structure Type: Unknown Reg\" + \":\" + str(url))\n",
    "        # Handle any unexpected structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Scrape Versions and return a dataframe\n",
    "def scrape_versions_to_df(driver, soup, url):\n",
    "    \"\"\"\n",
    "    Scrape versions of legislation from a web page and return them as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    driver: WebDriver: The Selenium WebDriver used for web scraping.\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML.\n",
    "    url (str): The URL of the legislation page.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the scraped legislation versions with columns:\n",
    "      - 'a_href': The URLs of legislation versions.\n",
    "      - 'valid_from': The valid from date of each version.\n",
    "      - 'valid_to': The valid to date of each version.\n",
    "\n",
    "    This function scrapes versions of legislation content from a web page and returns them as a DataFrame.\n",
    "    It processes the 'More Versions' link, extracts version details, and standardizes the URLs.\n",
    "    The DataFrame is structured with columns for the URL of each version, its valid from date, and valid to date.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    # Define a Selenium WebDriver and create a BeautifulSoup object\n",
    "    driver = webdriver.Chrome()\n",
    "    url = 'https://www.ontario.ca/laws/regulation/980134'\n",
    "    response = driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    # Scrape legislation versions and store them in a DataFrame\n",
    "    versions_df = scrape_versions_to_df(driver, soup, url)\n",
    "    driver.quit()\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(versions_df.head())\n",
    "    ```\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check for the presence of the 'More Versions' link\n",
    "        try:\n",
    "            link = driver.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tr.more-versions.hide a\")))\n",
    "            link.click()\n",
    "        except:\n",
    "            pass  # Suppressing print statements\n",
    "\n",
    "        # Find the 'div' tag with the class \"versions\"\n",
    "        versions_div = soup.find('div', id=\"versions\")\n",
    "        if not versions_div:\n",
    "            print(\"Versions Tag not found.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Find the first relevant table within the versions div\n",
    "        versions_table = versions_div.find('table', class_='act-reg-list noStripes')\n",
    "        if not versions_table:\n",
    "            print(\"Versions table not found.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Process each row in the first table\n",
    "        hrefs, valid_froms, valid_tos = [], [], []\n",
    "        for index, row in enumerate(versions_table.find_all('tr')):\n",
    "            td_cells = row.find_all('td')\n",
    "            if len(td_cells) >= 2:\n",
    "                a_tag = td_cells[1].find('a')\n",
    "                href = a_tag['href'] if a_tag else 'N/A'\n",
    "\n",
    "                # Replace the first href with the current url\n",
    "                if index == 0:\n",
    "                    hrefs.append(url)\n",
    "                else:\n",
    "                    hrefs.append(href)\n",
    "\n",
    "                # Check if current legislation\n",
    "                current_label = td_cells[0].find('span', class_='label')\n",
    "                if current_label and current_label.get_text().strip().lower() == 'current':\n",
    "                    valid_from = td_cells[1].find('span', class_='time').get_text().strip()\n",
    "                    valid_to = 'current'\n",
    "                else:\n",
    "                    # Extract valid dates\n",
    "                    span_tags = td_cells[1].find_all('span', class_='time')\n",
    "                    if len(span_tags) == 1:\n",
    "                        valid_from = 'Repealed'\n",
    "                        valid_to = span_tags[0].get_text().strip()\n",
    "                    elif len(span_tags) > 1:\n",
    "                        valid_from = span_tags[0].get_text().strip()\n",
    "                        valid_to = span_tags[1].get_text().strip()\n",
    "                    else:\n",
    "                        valid_from = 'N/A'\n",
    "                        valid_to = 'N/A'\n",
    "\n",
    "                valid_froms.append(valid_from)\n",
    "                valid_tos.append(valid_to)\n",
    "\n",
    "        # Create DataFrame and return it\n",
    "        versions_data = pd.DataFrame({\n",
    "            'a_href': hrefs,\n",
    "            'valid_from': valid_froms,\n",
    "            'valid_to': valid_tos\n",
    "        })\n",
    "\n",
    "        #Set first a_href to current url\n",
    "        versions_data['a_href'][0] = url\n",
    "\n",
    "        # Standardize URLs by removing the base URL part from first entry\n",
    "        base_url = \"https://www.ontario.ca\"\n",
    "        versions_data['a_href'][0] = versions_data['a_href'][0].replace(base_url, '')\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return versions_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the current state\n",
    "def save_state(index, file='last_state.txt'):\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the last saved state\n",
    "def load_last_successful_state(file='last_state.txt'):\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            return int(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Scrape Latest In Force Versions of Laws & Regs\n",
    "def scrape_latest_versions(url, start_page, end_page, step):\n",
    "    \"\"\"\n",
    "    Scrape and save the latest in-force versions of laws and regulations.\n",
    "\n",
    "    Args:\n",
    "    url (str): The base URL for scraping.\n",
    "    start_page (int): The index of the page to start scraping from.\n",
    "    end_page (int): The index of the page to stop scraping at.\n",
    "    step (int): The step size for iterating through pages.\n",
    "\n",
    "    This function scrapes the latest in-force versions of laws and regulations from the specified URL.\n",
    "    It loads or scrapes a DataFrame containing law and regulation information, then iterates through the rows.\n",
    "    For each row, it determines if it's an act or regulation, scrapes the versions, and saves the data.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    # Define the URL and scraping parameters\n",
    "    base_url = \"https://www.ontario.ca\"\n",
    "    start_page = 0\n",
    "    end_page = 10\n",
    "    step = 1\n",
    "\n",
    "    # Scrape and save the latest versions of laws and regulations\n",
    "    scrape_latest_versions(base_url, start_page, end_page, step)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    #Set Function Terms\n",
    "    base_url = \"https://www.ontario.ca\"\n",
    "    main_base_url = url\n",
    "    laws_and_regs_file = 'laws_and_regs.csv'\n",
    "\n",
    "    # Load or scrape the DataFrame\n",
    "    try:\n",
    "        laws_and_regs = pd.read_csv(laws_and_regs_file)\n",
    "        print(\"Loaded saved laws and regulations.\")\n",
    "    except FileNotFoundError:\n",
    "        laws_and_regs = scrape_ontario_laws(main_base_url, start_page=0, end_page=end_page, step=step)\n",
    "        laws_and_regs.to_csv(laws_and_regs_file, index=False)\n",
    "        print(\"Saved new laws and regulations.\")\n",
    "\n",
    "    # Iterate through each row, starting from start_page\n",
    "    for index, row in laws_and_regs.iterrows():\n",
    "        # Skip rows until the start_page index is reached\n",
    "        if index < start_page:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Complete URL\n",
    "            full_url = base_url + row['ahref']\n",
    "\n",
    "            driver = start_driver_and_wait(full_url)\n",
    "            soup = fetch_and_parse(driver)\n",
    "\n",
    "            # Act Logic\n",
    "            if row['class'] == 'act':\n",
    "                print(f\"Act at {full_url}\")\n",
    "                act_versions_df = scrape_versions_to_df(driver, soup, full_url)\n",
    "                valid_from = act_versions_df['valid_from'][0]\n",
    "                valid_to = act_versions_df['valid_to'][0]\n",
    "                scrape_law_page(driver, soup, full_url, valid_from, valid_to)\n",
    "\n",
    "            # Regulation Logic\n",
    "            elif row['class'] == 'regulation':\n",
    "                print(f\"Regulation at {full_url}\")\n",
    "                reg_versions_df = scrape_versions_to_df(driver, soup, full_url)\n",
    "                valid_from = reg_versions_df['valid_from'][0]\n",
    "                valid_to = reg_versions_df['valid_to'][0]\n",
    "                scrape_reg_page(driver, soup, full_url, valid_from, valid_to)\n",
    "                \n",
    "            # Save state after successful scraping\n",
    "            save_state(index)\n",
    "\n",
    "            # Quit the driver after processing each row\n",
    "            driver.quit()\n",
    "\n",
    "        except (AttributeError, TypeError) as e:\n",
    "            print(f\"Parsing error for {url}: {e}\")\n",
    "            continue  # Continue to the next row in case of an error\n",
    "\n",
    "        # Break the loop if end_page is reached\n",
    "        if index >= end_page:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Restartable Scraping\n",
    "def restartable_scrape(url, start_from=0, end_page=6000, step=50):\n",
    "    last_successful_index = load_last_successful_state()\n",
    "    while last_successful_index < end_page:\n",
    "        try:\n",
    "            scrape_latest_versions(url, last_successful_index, end_page, step)\n",
    "            break  # Exit loop if scrape_latest_versions completes without errors\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            last_successful_index = load_last_successful_state()  # Reload the last successful index\n",
    "            # Consider adding a delay or logic to prevent infinite loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart from the last successful state\n",
    "url = get_main_base_url(\"all_law\")\n",
    "restartable_scrape(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Draft All-Versions Scraping Script\n",
    "def scrape_all_versions:(url, start_page, end_page, step):\n",
    "    \"\"\"Draft All-Versions Scraping Script for Ontario Laws and Regulations\n",
    "\n",
    "    This script is designed to automate the process of scraping all versions of laws and regulations from the Ontario government's website. It focuses on systematically retrieving and storing information in a structured format, suitable for further analysis or archival purposes.\n",
    "\n",
    "    Functions:\n",
    "    - scrape_ontario_laws: Retrieves a list of laws and regulations from the main base URL.\n",
    "    - scrape_versions_to_df: Extracts different versions of a particular law or regulation and stores them in a DataFrame.\n",
    "    - scrape_law_page: Scrapes the content of a law from its URL and saves it in JSON format.\n",
    "    - scrape_reg_page: Similar to scrape_law_page but tailored for regulations.\n",
    "\n",
    "    Flow:\n",
    "    1. Set the base URLs for the main page and the listing of all laws and regulations.\n",
    "    2. Create a list of laws and regulations to be scraped by calling scrape_ontario_laws.\n",
    "    3. Iterate through each law or regulation:\n",
    "    - For each 'act', scrape all its versions and save the data.\n",
    "    - For each 'regulation', perform a similar scraping and saving process.\n",
    "    4. Handle exceptions and errors gracefully to ensure the script continues running even if some URLs fail to be scraped.\n",
    "\n",
    "    Usage:\n",
    "    This script is intended for data collection and archival purposes. It should be used in compliance with the website's terms of service, particularly regarding automated data collection.\n",
    "\n",
    "    Note:\n",
    "    This script is currently in a draft stage and may require further testing and refinement for full functionality.\n",
    "    '''\n",
    "'''\n",
    "    # Scraping Time\n",
    "\n",
    "    # Set Base URL\n",
    "    base_url = \"https://www.ontario.ca\"\n",
    "\n",
    "    #Set main_base_url for list of all laws and regs\n",
    "    main_base_url = \"https://www.ontario.ca/laws?search=&filteroption=current&filterstate%5B0%5D=current&filterstate%5B1%5D=rrs&filteryear=&source_type%5B0%5D=public&source_type%5B1%5D=regulation&pit_date=&filtertype=Statute%2CRegulation&sort=&sort_letter=&browse=on&from=\"\n",
    "\n",
    "    # Create list of all laws and regs to be scraped\n",
    "    laws_and_regs = scrape_ontario_laws(main_base_url, start_page=0, end_page=50, step=50)\n",
    "\n",
    "\n",
    "    # Iterate through each row in the DataFrame, save all versions to db folder\n",
    "    for index, row in laws_and_regs.iterrows():\n",
    "\n",
    "        try:\n",
    "            # Complete URL\n",
    "            full_url = base_url + row['ahref']\n",
    "\n",
    "            #Act Logic\n",
    "            if row['class'] == 'act':\n",
    "                # Perform actions for 'act'\n",
    "                print(f\"Act at {full_url}\")\n",
    "            \n",
    "            #Scrape Versions for Each Act\n",
    "                act_versions_df = scrape_versions_to_df(full_url)\n",
    "\n",
    "                #Iterate over versions for each act, scrape, and save\n",
    "                for index, row in act_versions_df.iterrows():\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        # Set url to scrape from versions list\n",
    "                        law_link = base_url + row['a_href']\n",
    "\n",
    "                        #Print scraped page\n",
    "                        print(f\"Scraping {law_link}\")\n",
    "\n",
    "                        #Scrape law_link & Save JSON\n",
    "                        scrape_law_page(law_link, row['valid_from'], row['valid_to'])\n",
    "\n",
    "                    except (Exception, AttributeError, TypeError) as e:\n",
    "                        print(f\"Error scraping {url}: {e}\")\n",
    "                    continue  # Skip to the next URL in the list\n",
    "                \n",
    "                \n",
    "            elif row['class'] == 'regulation':\n",
    "                # Perform actions for 'regulation'\n",
    "                print(f\"Regulation at {full_url}\")\n",
    "                # Add more code here for regulations\n",
    "\n",
    "                #Scrape Versions to dataframe\n",
    "                reg_versions_df = scrape_versions_to_df(full_url)\n",
    "            \n",
    "                #Iterate over versions for each act, scrape, and save\n",
    "                for index, row in reg_versions_df.iterrows():\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        # Set url to scrape from versions list\n",
    "                        reg_link = base_url + row['a_href']\n",
    "\n",
    "                        #Print scraped page\n",
    "                        print(f\"Scraping {reg_link}\")\n",
    "\n",
    "\n",
    "                        valid_from = row['valid_from']\n",
    "                        valid_to = row['valid_to']\n",
    "\n",
    "                        #Scrape reg_link & Save JSON\n",
    "                        scrape_reg_page(reg_link, valid_from, valid_to) \n",
    "\n",
    "                    except (Exception, AttributeError, TypeError) as e:\n",
    "                        print(f\"Error scraping {url}: {e}\")\n",
    "                    continue  # Skip to the next URL in the list\n",
    "\n",
    "            # Optionally, handle cases that are neither 'regulation' nor 'act'\n",
    "            else:\n",
    "                print(f\"Row {index} is neither regulation nor act\")\n",
    "            continue\n",
    "\n",
    "        except (Exception, AttributeError, TypeError) as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            continue  # Skip to the next URL in the list\n",
    "\n",
    "            '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
